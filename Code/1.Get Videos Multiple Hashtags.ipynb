{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42245439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import requests\n",
    "import json\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d677e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hashtag: kitchenutensil\tid: 1617315784495109\tvideos count: 161\n",
      "hashtag: kitchengoods\tid: 1673018142898177\tvideos count: 1016\n",
      "hashtag: cookwares\tid: 52838805\tvideos count: 147\n"
     ]
    }
   ],
   "source": [
    "# Get hashtag ids from a list of hashtags - hashtag id is used to retrive videos for the next step\n",
    "\n",
    "hashtag_names = ['kitchenutensil', 'kitchengoods','cookwares'] # Input hashtags\n",
    "\n",
    "# Copy and paste from TikTok Scraper\n",
    "get_hashid_url = \"https://tiktok-scraper2.p.rapidapi.com/hashtag/info\"\n",
    "headers = {\n",
    "\t\"X-RapidAPI-Key\": \"d5071c594cmsh21246a7ad1062eep1d8006jsnea062d6d3002\",\n",
    "\t\"X-RapidAPI-Host\": \"tiktok-scraper2.p.rapidapi.com\"\n",
    "}\n",
    "\n",
    "hashtag_info_l = []\n",
    "for name in hashtag_names: \n",
    "    querystring = {\"hashtag\":name}\n",
    "    hashid_response = requests.request(\"GET\", get_hashid_url, headers=headers, params=querystring)\n",
    "    \n",
    "    hashtag_json = json.loads(hashid_response.text)\n",
    "    hashid = hashtag_json['challengeInfo']['challenge']['id']\n",
    "    video_count = hashtag_json['challengeInfo']['stats']['videoCount']\n",
    "    \n",
    "    print('hashtag: '+ name + \"\\tid: \" + hashid + '\\tvideos count: ' + str(video_count))\n",
    "    hashtag_info_l.append((name,hashid,video_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d346bdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a method to get videos from one hashtag\n",
    "\n",
    "def getVideos(name, hashid, video_count):\n",
    "    \n",
    "    get_videos_url = \"https://tiktok-scraper2.p.rapidapi.com/hashtag/videos\"\n",
    "    \n",
    "    headers = {\n",
    "        \"X-RapidAPI-Key\": \"d5071c594cmsh21246a7ad1062eep1d8006jsnea062d6d3002\",\n",
    "        \"X-RapidAPI-Host\": \"tiktok-scraper2.p.rapidapi.com\"\n",
    "    }\n",
    "    \n",
    "    # Use video_count to identify pages \n",
    "    # There might be a bug because the API is not able to retrive all the videos from each hashtag\n",
    "    pages = math.ceil(video_count / 30) # math.ceil round up to the nearest integar\n",
    "    \n",
    "    # Create lists to store videos' information\n",
    "    video_id_l = []\n",
    "    video_desc_l = []\n",
    "    video_likes_count_l = []\n",
    "    video_comments_count_l = []\n",
    "    video_timestamp_l = []\n",
    "    video_url_l = []\n",
    "    video_hashtags_l = []\n",
    "    \n",
    "    # Page loop\n",
    "    for i in range(pages):\n",
    "        querystring = {\"hashtag_id\":hashid,\"cursor\":i*30}\n",
    "        response_videos = requests.request(\"GET\", get_videos_url, headers=headers, params=querystring)\n",
    "        json_videos = json.loads(response_videos.text)\n",
    "        \n",
    "        try:\n",
    "            video_items = json_videos['itemList']\n",
    "        \n",
    "            # On each page, get information of each video and save it to the lists created above\n",
    "            for video in video_items:\n",
    "                hashtags_l = []\n",
    "                \n",
    "                for hashtag_item in video['challenges']:\n",
    "                    hashtags_l.append(hashtag_item['title'])\n",
    "                \n",
    "                # Get information of each video (id, description, time, url,etc)\n",
    "                video_id = video['id']\n",
    "                video_desc = video['desc']\n",
    "                video_likes_count = video['stats']['diggCount']\n",
    "                video_comments_count = video['stats']['commentCount']\n",
    "                video_timestamp = video['createTime']\n",
    "                video_author = video['author']['uniqueId']\n",
    "                video_url = 'https://www.tiktok.com/@'+ video_author +'/video/'+ video_id # create video url using author name and video id\n",
    "                video_hashtags = ' '.join(hashtags_l)\n",
    "                \n",
    "                # Add video information to each list\n",
    "                video_id_l.append(video_id)\n",
    "                video_desc_l.append(video_desc)\n",
    "                video_likes_count_l.append(video_likes_count)\n",
    "                video_comments_count_l.append(video_comments_count)\n",
    "                video_timestamp_l.append(video_timestamp)\n",
    "                video_url_l.append(video_url)\n",
    "                video_hashtags_l.append(video_hashtags)\n",
    "                    \n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "        if json_videos == {}:\n",
    "            break\n",
    "            \n",
    "        if json_videos['hasMore'] == False: # this is initially designed to break when video extraction is completed\n",
    "            break\n",
    "            \n",
    "    # Save all videos information to a dictionary\n",
    "    video_dic = {'video_id':video_id_l,\n",
    "                 'video_desc':video_desc_l,\n",
    "                 'likes_count': video_likes_count_l,\n",
    "                 'comments_count': video_comments_count_l,\n",
    "                 'video_timestamp':video_timestamp_l,\n",
    "                 'video_url':video_url_l,\n",
    "                 'video_hashtags':video_hashtags_l}\n",
    "    \n",
    "    # Create a data frame for videos\n",
    "    video_df = pd.DataFrame(video_dic)\n",
    "    \n",
    "    return video_df\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "391ea691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some videos may be under more than one hashtag\n",
    "# Create a method to delate the duplicated videos \n",
    "\n",
    "def deleteDuplicates(df1, df2):\n",
    "    \n",
    "    # Create a list of the video ids in the previous data frame\n",
    "    id1_l = list(df1['video_id'])\n",
    "    \n",
    "    # Create a list of duplicated indexes\n",
    "    del_index_l = []\n",
    "    \n",
    "    for i,id2 in enumerate(df2['video_id']):\n",
    "        if str(id2) in id1_l:\n",
    "            del_index_l.append(i)\n",
    "    \n",
    "    # Drop duplicated rows\n",
    "    df2 = df2.drop(index=del_index_l)\n",
    "    \n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31a29ade",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'hasMore'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/8f/rpmbh_x517q304jkx3pxchgm0000gn/T/ipykernel_83369/2301242902.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mdf_sub\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetVideos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhashid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvideo_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mdf_sub\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_sub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"video_id\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'video_timestamp'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mdf_sub\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeleteDuplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_sub\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/8f/rpmbh_x517q304jkx3pxchgm0000gn/T/ipykernel_83369/103916060.py\u001b[0m in \u001b[0;36mgetVideos\u001b[0;34m(name, hashid, video_count)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mjson_videos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hasMore'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# this is initially designed to break when video extraction is completed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'hasMore'"
     ]
    }
   ],
   "source": [
    "# Get all videos from hashtag list\n",
    "\n",
    "for i,(name,hashid,video_count) in enumerate(hashtag_info_l):\n",
    "    \n",
    "    if i == 0:\n",
    "        video_df = getVideos(name,hashid,video_count)\n",
    "        # change long numbers to string format so the value will not be changed in the excel files\n",
    "        video_df = video_df.astype({\"video_id\": str,'video_timestamp':str})\n",
    "        video_df.to_excel(\"/Users/yuanyingmona/Documents/Study/UF/Courses/QMB6930 - Analytics Practicum/Data/videos_{}.xlsx\".format(name), sheet_name='videos', index=False)\n",
    "        \n",
    "    else:\n",
    "        df_sub = getVideos(name,hashid,video_count)\n",
    "        df_sub = df_sub.astype({\"video_id\": str,'video_timestamp':str})\n",
    "        df_sub = deleteDuplicates(video_df, df_sub)\n",
    "        \n",
    "        # Save videos dataframe seprately for getting comments\n",
    "        df_sub.to_excel(\"/Users/yuanyingmona/Documents/Study/UF/Courses/QMB6930 - Analytics Practicum/Data/videos_{}.xlsx\".format(name), sheet_name='videos', index=False)\n",
    "        \n",
    "        # Concat dfs \n",
    "        video_df = pd.concat([video_df,df_sub], ignore_index=True)\n",
    "        \n",
    "# Save dataframe of all videos to excel\n",
    "video_df.to_excel(\"/Users/yuanyingmona/Documents/Study/UF/Courses/QMB6930 - Analytics Practicum/Data/videos_all.xlsx\",sheet_name='videos', index=False) \n",
    "\n",
    "video_df\n",
    "\n",
    "# KeyError:'hasMore'\n",
    "# It is because the API result is changed. Can't find 'hasMore' anymore.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c34d145-a18a-462a-b350-19c163133b7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
